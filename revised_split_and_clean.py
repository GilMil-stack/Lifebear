# -*- coding: utf-8 -*-
"""Revised split and clean.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V4z5SAY8wUfrU0LnMmcH7ld7C4_9v0tG
"""

import pandas as pd
import re
import os

# File paths for input, output, and garbage files
input_file = '/content/Japan-2019.csv'
output_file = 'cleaned_lifebear.csv'
garbage_file = 'RemovedRecords.csv'
chunks = 6  # Number of chunks to split the output file

def extract(input_file):
    # Read the CSV file with semi-colon delimiter for the input file
    df = pd.read_csv(input_file, delimiter=';', encoding='utf-8')
    return df

def transform(df):
    # Initialize invalid rows DataFrame with an additional 'Issue' column
    invalid_rows = pd.DataFrame(columns=df.columns.tolist() + ['Issue'])

    # Clean invalid characters
    def clean_text(text):
        if isinstance(text, str):
            return re.sub(r'â€|[^\x00-\x7F]+', '', text)  # Removes non-ASCII characters
        return text

    # Validate email format
    def validate_email(email):
        if isinstance(email, str):
            if re.match(r"^[\w\.-]+@[\w\.-]+\.\w+$", email):  # Simple regex for email validation
                return email
            else:
                return None
        return email

    # Handle missing data by filling with 'none'
    def handle_missing(value):
        if pd.isnull(value) or value == '':
            return 'none'
        return value

    # Apply cleaning functions
    df_clean = df.copy()

    # Step 1: Clean text for all columns
    for column in df_clean.columns:
        df_clean[column] = df_clean[column].map(clean_text)

    # Step 2: Validate and clean email addresses
    if 'mail_address' in df_clean.columns:
        invalid_email_rows = df_clean[df_clean['mail_address'].map(validate_email).isnull()]
        if not invalid_email_rows.empty:
            invalid_email_rows['Issue'] = 'Invalid mail_address'
            invalid_rows = pd.concat([invalid_rows, invalid_email_rows], ignore_index=True)
        df_clean['mail_address'] = df_clean['mail_address'].map(validate_email)

    # Step 3: Handle missing values (fill with 'none')
    for column in df_clean.columns:
        df_clean[column] = df_clean[column].map(handle_missing)

    # Step 4: Remove rows where both login_id and mail_address are 'none'
    invalid_login_mail_rows = df_clean[(df_clean['login_id'] == 'none') & (df_clean['mail_address'] == 'none')]
    if not invalid_login_mail_rows.empty:
        invalid_login_mail_rows['Issue'] = 'Blank mail_address and login_id'
        invalid_rows = pd.concat([invalid_rows, invalid_login_mail_rows], ignore_index=True)
    df_clean = df_clean[~((df_clean['login_id'] == 'none') & (df_clean['mail_address'] == 'none'))]

    # Step 5: Handle duplicates by checking for duplicates in either login_id or mail_address
    duplicate_rows = df_clean[df_clean.duplicated(subset=['login_id', 'mail_address'], keep=False)]
    if not duplicate_rows.empty:
        duplicate_rows['Issue'] = 'duplicate'
        invalid_rows = pd.concat([invalid_rows, duplicate_rows], ignore_index=True)
    df_clean = df_clean.drop_duplicates(subset=['login_id', 'mail_address'], keep='first')

    # Remove invalid records from the cleaned DataFrame
    invalid_rows = invalid_rows.drop_duplicates()

    return df_clean, invalid_rows

def load(df_cleaned, output_file, invalid_rows, garbage_file):
    # Validate email format
    def validate_email(email):
        if isinstance(email, str):
            if re.match(r"^[\w\.-]+@[\w\.-]+\.\w+$", email):  # Simple regex for email validation
                return email
            else:
                return None
        return email

    # Save the cleaned DataFrame to a temporary CSV file
    temp_file = output_file.replace('.csv', '_nonduplicate.csv')
    df_cleaned.to_csv(temp_file, index=False, encoding='utf-8')

    # Save the invalid/garbage records to a separate CSV file
    invalid_rows.to_csv(garbage_file, index=False, encoding='utf-8')

    # Split the output file into chunks
    chunk_size = len(df_cleaned) // chunks + (len(df_cleaned) % chunks > 0)  # Calculate chunk size
    for i in range(chunks):
        split_df = df_cleaned.iloc[i * chunk_size:(i + 1) * chunk_size]
        if not split_df.empty:
            # Handle blank login_id and mail_address for garbage
            blank_rows = split_df[(split_df['login_id'] == 'none') & (split_df['mail_address'] == 'none')]
            if not blank_rows.empty:
                blank_rows['Issue'] = 'Blank mail_address and login_id'
                invalid_rows = pd.concat([invalid_rows, blank_rows], ignore_index=True)

            # Handle invalid email addresses
            if 'mail_address' in split_df.columns:
                invalid_email_rows = split_df[split_df['mail_address'].map(validate_email).isnull()]
                if not invalid_email_rows.empty:
                    invalid_email_rows['Issue'] = 'Invalid mail_address'
                    invalid_rows = pd.concat([invalid_rows, invalid_email_rows], ignore_index=True)

            # Save the cleaned split file
            split_file_name = f"{temp_file.replace('.csv', '')}_{i + 1}_cleaned.csv"
            split_df.to_csv(split_file_name, index=False, encoding='utf-8')

    # Once all split files have been processed, merge them into one file
    all_files = [f"{temp_file.replace('.csv', '')}_{i + 1}_cleaned.csv" for i in range(chunks)]
    merged_df = pd.concat([pd.read_csv(file) for file in all_files], ignore_index=True).drop_duplicates()
    merged_df.to_csv(output_file, index=False, encoding='utf-8')

# Main ETL Process
df_extracted = extract(input_file)
df_cleaned, df_invalid = transform(df_extracted)
load(df_cleaned, output_file, df_invalid, garbage_file)

import pandas as pd
import re
import os
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# File paths for input, output, and garbage files
input_file = '/content/Japan-2019.csv'
output_file = 'cleaned_lifebear.csv'
garbage_file = 'RemovedRecords.csv'
chunks = 6  # Number of chunks to split the output file

def extract(input_file):
    try:
        # Read the CSV file with semi-colon delimiter for the input file
        df = pd.read_csv(input_file, delimiter=';', encoding='utf-8')
        logging.info(f"Successfully extracted data from {input_file}.")
        return df
    except Exception as e:
        logging.error(f"Error reading the input file: {e}")
        raise

def transform(df):
    # Initialize invalid rows DataFrame with an Issue column
    invalid_rows = pd.DataFrame(columns=df.columns.tolist() + ['Issue'])

    # Clean invalid characters
    def clean_text(text):
        if isinstance(text, str):
            return re.sub(r'â€|[^\x00-\x7F]+', '', text)  # Removes non-ASCII characters
        return text

    # Validate email format
    def validate_email(email):
        if isinstance(email, str):
            if re.match(r"^[\w\.-]+@[\w\.-]+\.\w+$", email):  # Simple regex for email validation
                return email
            else:
                return None
        return email

    # Apply cleaning functions
    df_clean = df.copy()

    # Step 1: Clean text for all columns
    for column in df_clean.columns:
        df_clean[column] = df_clean[column].map(clean_text)

    # Step 2: Validate and clean email addresses
    if 'mail_address' in df_clean.columns:
        invalid_email_rows = df_clean[df_clean['mail_address'].map(validate_email).isnull()]
        if not invalid_email_rows.empty:
            invalid_email_rows['Issue'] = 'Invalid mail_address'
            invalid_rows = pd.concat([invalid_rows, invalid_email_rows], ignore_index=True)
        df_clean['mail_address'] = df_clean['mail_address'].map(validate_email)

    # Step 3: Remove rows where both login_id and mail_address are blank
    invalid_login_mail_rows = df_clean[(df_clean['login_id'].isnull() | (df_clean['login_id'] == '')) &
                                        (df_clean['mail_address'].isnull() | (df_clean['mail_address'] == ''))]
    if not invalid_login_mail_rows.empty:
        invalid_login_mail_rows['Issue'] = 'Blank mail_address and login_id'
        invalid_rows = pd.concat([invalid_rows, invalid_login_mail_rows], ignore_index=True)
    df_clean = df_clean[~((df_clean['login_id'].isnull() | (df_clean['login_id'] == '')) &
                          (df_clean['mail_address'].isnull() | (df_clean['mail_address'] == '')))]

    # Step 4: Handle duplicates by checking either login_id or mail_address
    duplicate_rows = df_clean[(df_clean['login_id'].duplicated(keep=False)) | (df_clean['mail_address'].duplicated(keep=False))]
    if not duplicate_rows.empty:
        duplicate_rows['Issue'] = 'duplicate'
        invalid_rows = pd.concat([invalid_rows, duplicate_rows], ignore_index=True)
    df_clean = df_clean.drop_duplicates(subset=['login_id', 'mail_address'], keep='first')

    # Remove invalid records from the cleaned DataFrame
    invalid_rows = invalid_rows.drop_duplicates()

    logging.info(f"Data transformation complete. Cleaned records: {len(df_clean)}, Invalid records: {len(invalid_rows)}.")
    return df_clean, invalid_rows

def load(df_cleaned, output_file, invalid_rows, garbage_file):
    try:
        # Validate email format
        def validate_email(email):
            if isinstance(email, str):
                if re.match(r"^[\w\.-]+@[\w\.-]+\.\w+$", email):  # Simple regex for email validation
                    return email
                else:
                    return None
            return email

        # Save the cleaned DataFrame to a temporary non-duplicate CSV file
        temp_file = os.path.splitext(output_file)[0] + '_nonduplicate.csv'
        df_cleaned.to_csv(temp_file, index=False, encoding='utf-8')
        logging.info(f"Temporary cleaned data saved to {temp_file}.")

        # Save the invalid/garbage records to a separate CSV file
        invalid_rows.to_csv(garbage_file, index=False, encoding='utf-8')
        logging.info(f"Invalid records saved to {garbage_file}.")

        # Split the output file into chunks
        chunk_size = len(df_cleaned) // chunks + (len(df_cleaned) % chunks > 0)  # Calculate chunk size

        for i in range(chunks):
            start_idx = i * chunk_size
            end_idx = start_idx + chunk_size
            chunk = df_cleaned.iloc[start_idx:end_idx]

            # Process each split file
            if not chunk.empty:
                # Remove time from 'created_at' if it exists
                if 'created_at' in chunk.columns:
                    chunk['created_at'] = pd.to_datetime(chunk['created_at']).dt.date

                # Write records where mail_address and login_id are blank to garbage file
                blank_records = chunk[(chunk['mail_address'].isnull() | (chunk['mail_address'] == '')) &
                                       (chunk['login_id'].isnull() | (chunk['login_id'] == ''))]
                if not blank_records.empty:
                    blank_records['Issue'] = 'Blank mail_address and login_id'
                    invalid_rows = pd.concat([invalid_rows, blank_records], ignore_index=True)

                # Write invalid mail_address records to garbage file
                invalid_mail_records = chunk[chunk['mail_address'].isnull()]
                if not invalid_mail_records.empty:
                    invalid_mail_records['Issue'] = 'Invalid mail_address'
                    invalid_rows = pd.concat([invalid_rows, invalid_mail_records], ignore_index=True)

                # Remove the blank records from the chunk
                chunk = chunk[~((chunk['mail_address'].isnull() | (chunk['mail_address'] == '')) &
                                (chunk['login_id'].isnull() | (chunk['login_id'] == '')))]

                # Save the cleaned split file
                chunk_file_name = f"{os.path.splitext(temp_file)[0]}_{i + 1}_cleaned.csv"
                chunk.to_csv(chunk_file_name, index=False, encoding='utf-8')
                logging.info(f"Chunk {i + 1} saved as {chunk_file_name}.")

        # Save the final garbage file after processing all splits
        invalid_rows.to_csv(garbage_file, index=False, encoding='utf-8')
        logging.info(f"Final invalid records saved to {garbage_file}.")

        # Merge all cleaned split files into one
        merged_file_path = output_file
        with open(merged_file_path, 'w', encoding='utf-8') as outfile:
            # Write the header for the first file
            first_chunk = True
            for i in range(chunks):
                chunk_file_name = f"{os.path.splitext(temp_file)[0]}_{i + 1}_cleaned.csv"
                if os.path.exists(chunk_file_name):
                    with open(chunk_file_name, 'r', encoding='utf-8') as infile:
                        if first_chunk:
                            outfile.write(infile.readline())  # Write header
                            first_chunk = False
                        infile.readline()  # Skip header
                        outfile.write(infile.read())  # Write the rest of the data

        logging.info(f"All chunks merged into {merged_file_path}.")

        # Optionally, remove split files after merging
        #for i in range(chunks):
        #    chunk_file_name = f"{os.path.splitext(temp_file)[0]}_{i + 1}_cleaned.csv"
        #    if os.path.exists(chunk_file_name):
        #        os.remove(chunk_file_name)
        #        logging.info(f"Removed temporary chunk file {chunk_file_name}.")

    except Exception as e:
        logging.error(f"Error during loading process: {e}")
        raise

# Main execution flow
if __name__ == "__main__":
    try:
        df = extract(input_file)
        df_cleaned, invalid_rows = transform(df)
        load(df_cleaned, output_file, invalid_rows, garbage_file)
        logging.info("Data processing completed successfully.")
    except Exception as e:
        logging.critical(f"Processing failed: {e}")